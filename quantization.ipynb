{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x151fa869170>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set up warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.ao.quantization'\n",
    ")\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNet(\n",
      "  (prelayer): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "  )\n",
      "  (a3): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(32, eps=32, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (b3): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(96, eps=96, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (a4): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(48, eps=48, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (b4): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (c4): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (d4): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (e4): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=128, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (a5): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=128, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (b5): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=128, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (dropout): Dropout2d(p=0.4, inplace=False)\n",
      "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import prune\n",
    "from collections import OrderedDict\n",
    "from models.googlenet import GoogleNet\n",
    "\n",
    "model = GoogleNet()\n",
    "model.load_state_dict(torch.load('googlenet-196-best.pth', map_location=torch.device('cpu')))\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['prelayer.0.weight', 'prelayer.1.weight', 'prelayer.1.bias', 'prelayer.1.running_mean', 'prelayer.1.running_var', 'prelayer.1.num_batches_tracked', 'prelayer.3.weight', 'prelayer.4.weight', 'prelayer.4.bias', 'prelayer.4.running_mean', 'prelayer.4.running_var', 'prelayer.4.num_batches_tracked', 'prelayer.6.weight', 'prelayer.7.weight', 'prelayer.7.bias', 'prelayer.7.running_mean', 'prelayer.7.running_var', 'prelayer.7.num_batches_tracked', 'a3.b1.0.weight', 'a3.b1.0.bias', 'a3.b1.1.weight', 'a3.b1.1.bias', 'a3.b1.1.running_mean', 'a3.b1.1.running_var', 'a3.b1.1.num_batches_tracked', 'a3.b2.0.weight', 'a3.b2.0.bias', 'a3.b2.1.weight', 'a3.b2.1.bias', 'a3.b2.1.running_mean', 'a3.b2.1.running_var', 'a3.b2.1.num_batches_tracked', 'a3.b2.3.weight', 'a3.b2.3.bias', 'a3.b2.4.weight', 'a3.b2.4.bias', 'a3.b2.4.running_mean', 'a3.b2.4.running_var', 'a3.b2.4.num_batches_tracked', 'a3.b3.0.weight', 'a3.b3.0.bias', 'a3.b3.1.weight', 'a3.b3.1.bias', 'a3.b3.1.running_mean', 'a3.b3.1.running_var', 'a3.b3.1.num_batches_tracked', 'a3.b3.3.weight', 'a3.b3.3.bias', 'a3.b3.4.weight', 'a3.b3.4.bias', 'a3.b3.4.running_mean', 'a3.b3.4.running_var', 'a3.b3.4.num_batches_tracked', 'a3.b3.6.weight', 'a3.b3.6.bias', 'a3.b3.7.weight', 'a3.b3.7.bias', 'a3.b3.7.running_mean', 'a3.b3.7.running_var', 'a3.b3.7.num_batches_tracked', 'a3.b4.1.weight', 'a3.b4.1.bias', 'a3.b4.2.weight', 'a3.b4.2.bias', 'a3.b4.2.running_mean', 'a3.b4.2.running_var', 'a3.b4.2.num_batches_tracked', 'b3.b1.0.weight', 'b3.b1.0.bias', 'b3.b1.1.weight', 'b3.b1.1.bias', 'b3.b1.1.running_mean', 'b3.b1.1.running_var', 'b3.b1.1.num_batches_tracked', 'b3.b2.0.weight', 'b3.b2.0.bias', 'b3.b2.1.weight', 'b3.b2.1.bias', 'b3.b2.1.running_mean', 'b3.b2.1.running_var', 'b3.b2.1.num_batches_tracked', 'b3.b2.3.weight', 'b3.b2.3.bias', 'b3.b2.4.weight', 'b3.b2.4.bias', 'b3.b2.4.running_mean', 'b3.b2.4.running_var', 'b3.b2.4.num_batches_tracked', 'b3.b3.0.weight', 'b3.b3.0.bias', 'b3.b3.1.weight', 'b3.b3.1.bias', 'b3.b3.1.running_mean', 'b3.b3.1.running_var', 'b3.b3.1.num_batches_tracked', 'b3.b3.3.weight', 'b3.b3.3.bias', 'b3.b3.4.weight', 'b3.b3.4.bias', 'b3.b3.4.running_mean', 'b3.b3.4.running_var', 'b3.b3.4.num_batches_tracked', 'b3.b3.6.weight', 'b3.b3.6.bias', 'b3.b3.7.weight', 'b3.b3.7.bias', 'b3.b3.7.running_mean', 'b3.b3.7.running_var', 'b3.b3.7.num_batches_tracked', 'b3.b4.1.weight', 'b3.b4.1.bias', 'b3.b4.2.weight', 'b3.b4.2.bias', 'b3.b4.2.running_mean', 'b3.b4.2.running_var', 'b3.b4.2.num_batches_tracked', 'a4.b1.0.weight', 'a4.b1.0.bias', 'a4.b1.1.weight', 'a4.b1.1.bias', 'a4.b1.1.running_mean', 'a4.b1.1.running_var', 'a4.b1.1.num_batches_tracked', 'a4.b2.0.weight', 'a4.b2.0.bias', 'a4.b2.1.weight', 'a4.b2.1.bias', 'a4.b2.1.running_mean', 'a4.b2.1.running_var', 'a4.b2.1.num_batches_tracked', 'a4.b2.3.weight', 'a4.b2.3.bias', 'a4.b2.4.weight', 'a4.b2.4.bias', 'a4.b2.4.running_mean', 'a4.b2.4.running_var', 'a4.b2.4.num_batches_tracked', 'a4.b3.0.weight', 'a4.b3.0.bias', 'a4.b3.1.weight', 'a4.b3.1.bias', 'a4.b3.1.running_mean', 'a4.b3.1.running_var', 'a4.b3.1.num_batches_tracked', 'a4.b3.3.weight', 'a4.b3.3.bias', 'a4.b3.4.weight', 'a4.b3.4.bias', 'a4.b3.4.running_mean', 'a4.b3.4.running_var', 'a4.b3.4.num_batches_tracked', 'a4.b3.6.weight', 'a4.b3.6.bias', 'a4.b3.7.weight', 'a4.b3.7.bias', 'a4.b3.7.running_mean', 'a4.b3.7.running_var', 'a4.b3.7.num_batches_tracked', 'a4.b4.1.weight', 'a4.b4.1.bias', 'a4.b4.2.weight', 'a4.b4.2.bias', 'a4.b4.2.running_mean', 'a4.b4.2.running_var', 'a4.b4.2.num_batches_tracked', 'b4.b1.0.weight', 'b4.b1.0.bias', 'b4.b1.1.weight', 'b4.b1.1.bias', 'b4.b1.1.running_mean', 'b4.b1.1.running_var', 'b4.b1.1.num_batches_tracked', 'b4.b2.0.weight', 'b4.b2.0.bias', 'b4.b2.1.weight', 'b4.b2.1.bias', 'b4.b2.1.running_mean', 'b4.b2.1.running_var', 'b4.b2.1.num_batches_tracked', 'b4.b2.3.weight', 'b4.b2.3.bias', 'b4.b2.4.weight', 'b4.b2.4.bias', 'b4.b2.4.running_mean', 'b4.b2.4.running_var', 'b4.b2.4.num_batches_tracked', 'b4.b3.0.weight', 'b4.b3.0.bias', 'b4.b3.1.weight', 'b4.b3.1.bias', 'b4.b3.1.running_mean', 'b4.b3.1.running_var', 'b4.b3.1.num_batches_tracked', 'b4.b3.3.weight', 'b4.b3.3.bias', 'b4.b3.4.weight', 'b4.b3.4.bias', 'b4.b3.4.running_mean', 'b4.b3.4.running_var', 'b4.b3.4.num_batches_tracked', 'b4.b3.6.weight', 'b4.b3.6.bias', 'b4.b3.7.weight', 'b4.b3.7.bias', 'b4.b3.7.running_mean', 'b4.b3.7.running_var', 'b4.b3.7.num_batches_tracked', 'b4.b4.1.weight', 'b4.b4.1.bias', 'b4.b4.2.weight', 'b4.b4.2.bias', 'b4.b4.2.running_mean', 'b4.b4.2.running_var', 'b4.b4.2.num_batches_tracked', 'c4.b1.0.weight', 'c4.b1.0.bias', 'c4.b1.1.weight', 'c4.b1.1.bias', 'c4.b1.1.running_mean', 'c4.b1.1.running_var', 'c4.b1.1.num_batches_tracked', 'c4.b2.0.weight', 'c4.b2.0.bias', 'c4.b2.1.weight', 'c4.b2.1.bias', 'c4.b2.1.running_mean', 'c4.b2.1.running_var', 'c4.b2.1.num_batches_tracked', 'c4.b2.3.weight', 'c4.b2.3.bias', 'c4.b2.4.weight', 'c4.b2.4.bias', 'c4.b2.4.running_mean', 'c4.b2.4.running_var', 'c4.b2.4.num_batches_tracked', 'c4.b3.0.weight', 'c4.b3.0.bias', 'c4.b3.1.weight', 'c4.b3.1.bias', 'c4.b3.1.running_mean', 'c4.b3.1.running_var', 'c4.b3.1.num_batches_tracked', 'c4.b3.3.weight', 'c4.b3.3.bias', 'c4.b3.4.weight', 'c4.b3.4.bias', 'c4.b3.4.running_mean', 'c4.b3.4.running_var', 'c4.b3.4.num_batches_tracked', 'c4.b3.6.weight', 'c4.b3.6.bias', 'c4.b3.7.weight', 'c4.b3.7.bias', 'c4.b3.7.running_mean', 'c4.b3.7.running_var', 'c4.b3.7.num_batches_tracked', 'c4.b4.1.weight', 'c4.b4.1.bias', 'c4.b4.2.weight', 'c4.b4.2.bias', 'c4.b4.2.running_mean', 'c4.b4.2.running_var', 'c4.b4.2.num_batches_tracked', 'd4.b1.0.weight', 'd4.b1.0.bias', 'd4.b1.1.weight', 'd4.b1.1.bias', 'd4.b1.1.running_mean', 'd4.b1.1.running_var', 'd4.b1.1.num_batches_tracked', 'd4.b2.0.weight', 'd4.b2.0.bias', 'd4.b2.1.weight', 'd4.b2.1.bias', 'd4.b2.1.running_mean', 'd4.b2.1.running_var', 'd4.b2.1.num_batches_tracked', 'd4.b2.3.weight', 'd4.b2.3.bias', 'd4.b2.4.weight', 'd4.b2.4.bias', 'd4.b2.4.running_mean', 'd4.b2.4.running_var', 'd4.b2.4.num_batches_tracked', 'd4.b3.0.weight', 'd4.b3.0.bias', 'd4.b3.1.weight', 'd4.b3.1.bias', 'd4.b3.1.running_mean', 'd4.b3.1.running_var', 'd4.b3.1.num_batches_tracked', 'd4.b3.3.weight', 'd4.b3.3.bias', 'd4.b3.4.weight', 'd4.b3.4.bias', 'd4.b3.4.running_mean', 'd4.b3.4.running_var', 'd4.b3.4.num_batches_tracked', 'd4.b3.6.weight', 'd4.b3.6.bias', 'd4.b3.7.weight', 'd4.b3.7.bias', 'd4.b3.7.running_mean', 'd4.b3.7.running_var', 'd4.b3.7.num_batches_tracked', 'd4.b4.1.weight', 'd4.b4.1.bias', 'd4.b4.2.weight', 'd4.b4.2.bias', 'd4.b4.2.running_mean', 'd4.b4.2.running_var', 'd4.b4.2.num_batches_tracked', 'e4.b1.0.weight', 'e4.b1.0.bias', 'e4.b1.1.weight', 'e4.b1.1.bias', 'e4.b1.1.running_mean', 'e4.b1.1.running_var', 'e4.b1.1.num_batches_tracked', 'e4.b2.0.weight', 'e4.b2.0.bias', 'e4.b2.1.weight', 'e4.b2.1.bias', 'e4.b2.1.running_mean', 'e4.b2.1.running_var', 'e4.b2.1.num_batches_tracked', 'e4.b2.3.weight', 'e4.b2.3.bias', 'e4.b2.4.weight', 'e4.b2.4.bias', 'e4.b2.4.running_mean', 'e4.b2.4.running_var', 'e4.b2.4.num_batches_tracked', 'e4.b3.0.weight', 'e4.b3.0.bias', 'e4.b3.1.weight', 'e4.b3.1.bias', 'e4.b3.1.running_mean', 'e4.b3.1.running_var', 'e4.b3.1.num_batches_tracked', 'e4.b3.3.weight', 'e4.b3.3.bias', 'e4.b3.4.weight', 'e4.b3.4.bias', 'e4.b3.4.running_mean', 'e4.b3.4.running_var', 'e4.b3.4.num_batches_tracked', 'e4.b3.6.weight', 'e4.b3.6.bias', 'e4.b3.7.weight', 'e4.b3.7.bias', 'e4.b3.7.running_mean', 'e4.b3.7.running_var', 'e4.b3.7.num_batches_tracked', 'e4.b4.1.weight', 'e4.b4.1.bias', 'e4.b4.2.weight', 'e4.b4.2.bias', 'e4.b4.2.running_mean', 'e4.b4.2.running_var', 'e4.b4.2.num_batches_tracked', 'a5.b1.0.weight', 'a5.b1.0.bias', 'a5.b1.1.weight', 'a5.b1.1.bias', 'a5.b1.1.running_mean', 'a5.b1.1.running_var', 'a5.b1.1.num_batches_tracked', 'a5.b2.0.weight', 'a5.b2.0.bias', 'a5.b2.1.weight', 'a5.b2.1.bias', 'a5.b2.1.running_mean', 'a5.b2.1.running_var', 'a5.b2.1.num_batches_tracked', 'a5.b2.3.weight', 'a5.b2.3.bias', 'a5.b2.4.weight', 'a5.b2.4.bias', 'a5.b2.4.running_mean', 'a5.b2.4.running_var', 'a5.b2.4.num_batches_tracked', 'a5.b3.0.weight', 'a5.b3.0.bias', 'a5.b3.1.weight', 'a5.b3.1.bias', 'a5.b3.1.running_mean', 'a5.b3.1.running_var', 'a5.b3.1.num_batches_tracked', 'a5.b3.3.weight', 'a5.b3.3.bias', 'a5.b3.4.weight', 'a5.b3.4.bias', 'a5.b3.4.running_mean', 'a5.b3.4.running_var', 'a5.b3.4.num_batches_tracked', 'a5.b3.6.weight', 'a5.b3.6.bias', 'a5.b3.7.weight', 'a5.b3.7.bias', 'a5.b3.7.running_mean', 'a5.b3.7.running_var', 'a5.b3.7.num_batches_tracked', 'a5.b4.1.weight', 'a5.b4.1.bias', 'a5.b4.2.weight', 'a5.b4.2.bias', 'a5.b4.2.running_mean', 'a5.b4.2.running_var', 'a5.b4.2.num_batches_tracked', 'b5.b1.0.weight', 'b5.b1.0.bias', 'b5.b1.1.weight', 'b5.b1.1.bias', 'b5.b1.1.running_mean', 'b5.b1.1.running_var', 'b5.b1.1.num_batches_tracked', 'b5.b2.0.weight', 'b5.b2.0.bias', 'b5.b2.1.weight', 'b5.b2.1.bias', 'b5.b2.1.running_mean', 'b5.b2.1.running_var', 'b5.b2.1.num_batches_tracked', 'b5.b2.3.weight', 'b5.b2.3.bias', 'b5.b2.4.weight', 'b5.b2.4.bias', 'b5.b2.4.running_mean', 'b5.b2.4.running_var', 'b5.b2.4.num_batches_tracked', 'b5.b3.0.weight', 'b5.b3.0.bias', 'b5.b3.1.weight', 'b5.b3.1.bias', 'b5.b3.1.running_mean', 'b5.b3.1.running_var', 'b5.b3.1.num_batches_tracked', 'b5.b3.3.weight', 'b5.b3.3.bias', 'b5.b3.4.weight', 'b5.b3.4.bias', 'b5.b3.4.running_mean', 'b5.b3.4.running_var', 'b5.b3.4.num_batches_tracked', 'b5.b3.6.weight', 'b5.b3.6.bias', 'b5.b3.7.weight', 'b5.b3.7.bias', 'b5.b3.7.running_mean', 'b5.b3.7.running_var', 'b5.b3.7.num_batches_tracked', 'b5.b4.1.weight', 'b5.b4.1.bias', 'b5.b4.2.weight', 'b5.b4.2.bias', 'b5.b4.2.running_mean', 'b5.b4.2.running_var', 'b5.b4.2.num_batches_tracked', 'linear.weight', 'linear.bias'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of GoogleNet(\n",
       "  (prelayer): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (a3): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(32, eps=32, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (b3): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(96, eps=96, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (a4): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(48, eps=48, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (b4): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (c4): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (d4): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (e4): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=128, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (a5): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=128, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (b5): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=128, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout2d(p=0.4, inplace=False)\n",
       "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for name, module in model.named_modules():\\n    for mini_name, mini_module in module.named_modules():\\n        # prune 20% of connections in all 2D-conv layers\\n        if isinstance(mini_module, torch.nn.Conv2d):\\n            print(f\"Sparsity in {mini_name} before: {100. * float(torch.sum(mini_module.weight == 0))/ float(mini_module.weight.nelement()):.2f}%\")\\n            prune.l1_unstructured(mini_module, name=\\'weight\\', amount=0.4)\\n            prune.remove(mini_module, \\'weight\\')\\n            print(f\"Sparsity in {mini_name} after: {100. * float(torch.sum(mini_module.weight == 0))/ float(mini_module.weight.nelement()):.2f}%\")\\n        # prune 40% of connections in all linear layers\\n\\nprint(dict(model.named_buffers()).keys())  # to verify that all masks exist'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for name, module in model.named_modules():\n",
    "    for mini_name, mini_module in module.named_modules():\n",
    "        # prune 20% of connections in all 2D-conv layers\n",
    "        if isinstance(mini_module, torch.nn.Conv2d):\n",
    "            print(f\"Sparsity in {mini_name} before: {100. * float(torch.sum(mini_module.weight == 0))/ float(mini_module.weight.nelement()):.2f}%\")\n",
    "            prune.l1_unstructured(mini_module, name='weight', amount=0.4)\n",
    "            prune.remove(mini_module, 'weight')\n",
    "            print(f\"Sparsity in {mini_name} after: {100. * float(torch.sum(mini_module.weight == 0))/ float(mini_module.weight.nelement()):.2f}%\")\n",
    "        # prune 40% of connections in all linear layers\n",
    "\n",
    "print(dict(model.named_buffers()).keys())  # to verify that all masks exist'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_to_fuse = [\n",
    "    ['prelayer.0', 'prelayer.1', 'prelayer.2'],\n",
    "    ['prelayer.3', 'prelayer.4', 'prelayer.5'],\n",
    "    ['prelayer.6', 'prelayer.7', 'prelayer.8'],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization using Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import argparse\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from conf import settings\n",
    "from utils import get_network, get_test_dataloader\n",
    "\n",
    "cifar100_test_loader = get_test_dataloader(\n",
    "        settings.CIFAR100_TRAIN_MEAN,\n",
    "        settings.CIFAR100_TRAIN_STD,\n",
    "        #settings.CIFAR100_PATH,\n",
    "        num_workers=4,\n",
    "        batch_size=16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DELL\\Documents\\GitHub\\cinnamon-week5\\quantization.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/GitHub/cinnamon-week5/quantization.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         loss \u001b[39m=\u001b[39m loss_fn(out, torch\u001b[39m.\u001b[39mrand_like(out))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/GitHub/cinnamon-week5/quantization.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/GitHub/cinnamon-week5/quantization.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/GitHub/cinnamon-week5/quantization.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         opt\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/GitHub/cinnamon-week5/quantization.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Convert\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.quantized_googlenet import Quantized_Googlenet\n",
    "\n",
    "quantized_model = Quantized_Googlenet()\n",
    "quantized_model.load_state_dict(torch.load('googlenet-196-best.pth', map_location=torch.device('cpu')))\n",
    "quantized_model.eval()\n",
    "torch.quantization.fuse_modules(quantized_model, modules_to_fuse, inplace=True)\n",
    "\n",
    "# Prepare\n",
    "quantized_model.train()\n",
    "quantized_model.prelayer.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Training Loop\"\"\"\n",
    "n_epochs = 10\n",
    "opt = torch.optim.SGD(quantized_model.parameters(), lr=0.1)\n",
    "loss_fn = lambda out, tgt: torch.pow(tgt-out, 2).mean()\n",
    "for epoch in range(n_epochs):\n",
    "    for n_iter, (image, label) in enumerate(cifar100_test_loader):\n",
    "        out = quantized_model(image)\n",
    "        loss = loss_fn(out, torch.rand_like(out))\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "# Convert\n",
    "quantized_model.eval()\n",
    "torch.quantization.convert(quantized_model, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Quantized_Googlenet(\n",
       "  (prelayer): Sequential(\n",
       "    (0): QuantizedConvReLU2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.009797699749469757, zero_point=0, padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Identity()\n",
       "    (3): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.01079961284995079, zero_point=0, padding=(1, 1))\n",
       "    (4): Identity()\n",
       "    (5): Identity()\n",
       "    (6): QuantizedConvReLU2d(64, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.006309532560408115, zero_point=0, padding=(1, 1))\n",
       "    (7): Identity()\n",
       "    (8): Identity()\n",
       "  )\n",
       "  (a3): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(32, eps=32, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (b3): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(96, eps=96, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (a4): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(48, eps=48, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (b4): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (c4): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (d4): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (e4): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=128, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (a5): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=128, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (b5): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=128, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout2d(p=0.4, inplace=False)\n",
       "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(quantized_model.state_dict(), 'quantized_googlenet1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Quantized_Googlenet(\n",
      "  (prelayer): Sequential(\n",
      "    (0): QuantizedConvReLU2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.009797699749469757, zero_point=0, padding=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "    (3): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.01079961284995079, zero_point=0, padding=(1, 1))\n",
      "    (4): Identity()\n",
      "    (5): Identity()\n",
      "    (6): QuantizedConvReLU2d(64, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.006309532560408115, zero_point=0, padding=(1, 1))\n",
      "    (7): Identity()\n",
      "    (8): Identity()\n",
      "  )\n",
      "  (a3): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(32, eps=32, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (b3): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(96, eps=96, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (a4): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(48, eps=48, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (b4): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (c4): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (d4): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (e4): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=128, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (a5): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=128, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (b5): Inception(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=128, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (dropout): Dropout2d(p=0.4, inplace=False)\n",
      "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "iteration: 1\ttotal 625 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\profiler.py:179: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::conv2d_relu.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d_relu.new' is only available for these backends: [QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nQuantizedCPU: registered at ..\\aten\\src\\ATen\\native\\quantized\\cpu\\qconv.cpp:1555 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:30 [backend fallback]\nAutogradCPU: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:34 [backend fallback]\nAutogradCUDA: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:42 [backend fallback]\nAutogradXLA: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:46 [backend fallback]\nAutogradMPS: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:54 [backend fallback]\nAutogradXPU: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:38 [backend fallback]\nAutogradHPU: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradLazy: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:50 [backend fallback]\nAutogradMeta: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:58 [backend fallback]\nTracer: registered at ..\\torch\\csrc\\autograd\\TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:148 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DELL\\Documents\\GitHub\\cinnamon-week5\\quantization.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/GitHub/cinnamon-week5/quantization.ipynb#X15sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mwith\u001b[39;00m profile(activities\u001b[39m=\u001b[39m[ProfilerActivity\u001b[39m.\u001b[39mCPU, ProfilerActivity\u001b[39m.\u001b[39mCUDA], profile_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m prof:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/GitHub/cinnamon-week5/quantization.ipynb#X15sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mwith\u001b[39;00m record_function(\u001b[39m\"\u001b[39m\u001b[39mmodel_inference\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/GitHub/cinnamon-week5/quantization.ipynb#X15sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         output \u001b[39m=\u001b[39m quantized_model(image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/GitHub/cinnamon-week5/quantization.ipynb#X15sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m#torch.cuda.synchronize()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL/Documents/GitHub/cinnamon-week5/quantization.ipynb#X15sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m _, pred \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mtopk(\u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m, largest\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39msorted\u001b[39m\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\DELL\\Documents\\GitHub\\cinnamon-week5\\models\\quantized_googlenet.py:102\u001b[0m, in \u001b[0;36mQuantized_Googlenet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    100\u001b[0m     \u001b[39m#x = torch.quantize_per_tensor(x, 0.1, 10, torch.quint8)\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mquantization\u001b[39m.\u001b[39mQuantStub()(x)\n\u001b[1;32m--> 102\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprelayer(x)\n\u001b[0;32m    103\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mquantization\u001b[39m.\u001b[39mDeQuantStub()(x)\n\u001b[0;32m    104\u001b[0m     \u001b[39m#x = torch.dequantize(x)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\nn\\intrinsic\\quantized\\modules\\conv_relu.py:98\u001b[0m, in \u001b[0;36mConvReLU2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     95\u001b[0m     _reversed_padding_repeated_twice \u001b[39m=\u001b[39m _reverse_repeat_padding(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding)\n\u001b[0;32m     96\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, _reversed_padding_repeated_twice,\n\u001b[0;32m     97\u001b[0m                   mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode)\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mquantized\u001b[39m.\u001b[39;49mconv2d_relu(\n\u001b[0;32m     99\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_packed_params, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzero_point)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    498\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_op(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs \u001b[39mor\u001b[39;00m {})\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'quantized::conv2d_relu.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d_relu.new' is only available for these backends: [QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nQuantizedCPU: registered at ..\\aten\\src\\ATen\\native\\quantized\\cpu\\qconv.cpp:1555 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:30 [backend fallback]\nAutogradCPU: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:34 [backend fallback]\nAutogradCUDA: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:42 [backend fallback]\nAutogradXLA: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:46 [backend fallback]\nAutogradMPS: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:54 [backend fallback]\nAutogradXPU: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:38 [backend fallback]\nAutogradHPU: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradLazy: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:50 [backend fallback]\nAutogradMeta: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:58 [backend fallback]\nTracer: registered at ..\\torch\\csrc\\autograd\\TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:148 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(quantized_model)\n",
    "\n",
    "correct_1 = 0.0\n",
    "correct_5 = 0.0\n",
    "total = 0\n",
    "total_time = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for n_iter, (image, label) in enumerate(cifar100_test_loader):\n",
    "        print(\"iteration: {}\\ttotal {} iterations\".format(n_iter + 1, len(cifar100_test_loader)))\n",
    "\n",
    "        # Measure time\n",
    "        with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True) as prof:\n",
    "            with record_function(\"model_inference\"):\n",
    "                output = quantized_model(image)\n",
    "        \n",
    "        #torch.cuda.synchronize()\n",
    "\n",
    "        _, pred = output.topk(5, 1, largest=True, sorted=True)\n",
    "\n",
    "        label = label.view(label.size(0), -1).expand_as(pred)\n",
    "        correct = pred.eq(label).float()\n",
    "\n",
    "        #compute top 5\n",
    "        correct_5 += correct[:, :5].sum()\n",
    "\n",
    "        #compute top1\n",
    "        correct_1 += correct[:, :1].sum()\n",
    "print()\n",
    "print(\"Top 1 err: \", 1 - correct_1 / len(cifar100_test_loader.dataset))\n",
    "print(\"Top 5 err: \", 1 - correct_5 / len(cifar100_test_loader.dataset))\n",
    "print(f\"Average inference time per image:miliseconds\")\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
